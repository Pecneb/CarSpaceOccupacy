# CarSpaceOccupacy
IACV Project implementation, which encapsulates car space occupacy analysis with image processing techniques.

### **Project Description: Car Motion and Space Occupancy Analysis in the Dark**

#### **Overview**
This project focuses on developing a computer vision system that analyzes a video of a moving vehicle taken by a fixed camera. The primary goal is to determine the **space occupied by the vehicle** in each frame while accounting for different motion conditions, including forward translation and steering at constant curvature. The system utilizes **camera calibration data** and a **simplified vehicle model** to estimate the vehicleâ€™s 3D position on the road.

#### **Key Components**
1. **Camera Calibration:** The intrinsic parameters of the camera (K-matrix) are provided and used for geometric calculations.
2. **Vehicle Model:** A simplified model of the car includes:
   - Car width and length.
   - Distance between the rear lights.
   - Height of rear lights from the ground.
   - Distance of the rear lights from the back of the car.

#### **Assumptions**
- The moving car has a **vertical symmetry plane**.
- Two **symmetric rear lights** are visible.
- The camera observes the **back of the vehicle**.
- The road is **locally planar**.
- Between consecutive frames, the car either:
  - Translates **forwards**.
  - **Steers** with a constant curvature.

#### **System Operation**
##### **Offline Steps**
1. **Camera Calibration** to retrieve intrinsic parameters.
2. **Vehicle Model Extraction**, including dimensions and light placements.

##### **Online Steps (Per Frame Analysis)**
1. **Feature Extraction:** Identify the two symmetric rear lights (or distinct features on them).
2. **Geometric Analysis:** Determine the **3D position** of the light segment and its containing plane (Ï€).
3. **Space Occupancy Estimation:** Using the car model and computed positions, estimate the **occupied space** on the road.

#### **Geometric Principles Used**
1. **Forward Translation:**
   - If the car is moving straight, the **first and second light segments** form a **rectangle**.
   - The system finds **vanishing points** (Vx, Vy) and the **vanishing line (l)**.
   - 3D localization of the light segments is achieved using geometric constraints (e.g., **theorem of cosines**).

2. **Steering with Constant Curvature:**
   - The light segment rotates about a **center of rotation (C)**.
   - The method estimates **vanishing points** and **perpendicular constraints** to determine motion curvature.

#### **Challenges & Solutions**
1. **Poor Perspective (Parallel Viewing Rays)**
   - The method requires **sufficient perspective effects** to work.
   - If perspective is weak, additional **non-coplanar symmetric elements** are used.
   - The system estimates the carâ€™s **pose** using an **iterative refinement** approach.

2. **Car Localization Methods**
   - **Standard Homography-Based Localization:** Uses **license plate corners and light features**.
   - **Nighttime Localization from Image Pairs:** Uses **rear lights** to estimate motion.
   - **Localization with Out-of-Plane Features:** When poor perspective makes angle estimation unreliable, the **difference in symmetric elements on separate planes** is used.

3. **Iterative Refinement for Pose Estimation**
   - **Refines estimates** of direction and position.
   - Updates **vanishing points** and **horizontal vanishing lines**.
   - Uses **camera calibration matrix** to improve estimates.

### **Conclusion**
The system provides an efficient **3D reconstruction of vehicle motion** based on **video analysis**, ensuring accurate **space occupancy estimation** in **low-light conditions**. This has potential applications in **traffic monitoring, autonomous vehicle navigation, and night-time vehicle tracking**.


Since we are implementing the **space occupancy analysis system in Python**, it's best to follow a **modular, scalable project structure** that allows clean separation between components like camera calibration, feature extraction, geometric computations, and visualization.

Directory structure guide:

```
space_occupancy/
â”‚
â”œâ”€â”€ data/                         # All raw video inputs, calibration data, and sample frames
â”‚   â”œâ”€â”€ videos/
â”‚   â”œâ”€â”€ calibration/
â”‚   â””â”€â”€ sample_frames/
â”‚
â”œâ”€â”€ configs/                      # Configuration files (e.g., camera matrix, car model)
â”‚   â”œâ”€â”€ camera_intrinsics.yaml
â”‚   â””â”€â”€ car_model.yaml
â”‚
â”œâ”€â”€ src/                          # All source code modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                   # Entry point
â”‚   â”œâ”€â”€ camera/                   # Camera calibration & projection logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ calibration.py
â”‚   â”‚   â””â”€â”€ projection.py
â”‚   â”‚
â”‚   â”œâ”€â”€ detection/                # Feature and light detection logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ light_detector.py
â”‚   â”‚   â””â”€â”€ feature_tracking.py
â”‚   â”‚
â”‚   â”œâ”€â”€ geometry/                 # 3D geometry, vanishing points, pose estimation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ space_occupancy.py
â”‚   â”‚   â”œâ”€â”€ plane_estimation.py
â”‚   â”‚   â””â”€â”€ vanishing_points.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                    # Helper functions (I/O, drawing, math)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ io_utils.py
â”‚   â”‚   â”œâ”€â”€ drawing.py
â”‚   â”‚   â””â”€â”€ math_utils.py
â”‚   â”‚
â”‚   â””â”€â”€ visualization/           # Plotting, annotated videos, image output
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ viewer.py
â”‚       â””â”€â”€ video_writer.py
â”‚
â”œâ”€â”€ notebooks/                   # Jupyter notebooks for testing and visualization
â”‚   â””â”€â”€ debug_pose_estimation.ipynb
â”‚
â”œâ”€â”€ tests/                       # Unit and integration tests
â”‚   â”œâ”€â”€ test_geometry.py
â”‚   â””â”€â”€ test_light_detection.py
â”‚
â”œâ”€â”€ results/                     # Outputs: 3D reconstructions, videos, logs
â”‚   â”œâ”€â”€ plots/
â”‚   â””â”€â”€ annotated_videos/
â”‚
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

### ğŸ“Œ Suggestions for Implementation
- Use **OpenCV** (`cv2`) for image I/O and basic feature detection.
- Use **NumPy** for matrix math and 3D operations.
- Consider **SciPy** or **OpenCV's solvePnP** for pose estimation.
- Store camera intrinsics and car model parameters in `.yaml` or `.json` files under `configs/`.
- Add **logging** and debugging support (e.g., timestamps, feature match quality).

Would you like me to generate a `requirements.txt` and a sample config file (like `camera_intrinsics.yaml`) to start from?


For the following photo we follow the steps below to extract useful information for the algorithm. Our target is the red kia sportage.

![Red Kia Sportage](/data/sample_frames/red_kia_sportage/frame_175.jpg)

---

## âœ… **1. Assign Real-World Coordinates to Key Points**
Using the **physical dimensions**:
- Rear lights are symmetric, so choose two reference points on them â€” e.g., centers of each light.
- Approximate the **rear light distance** from the specs:
  - If rear track is 1636 mm and light centers are slightly inset, estimate ~1500 mm between rear light centers.
- Assume the rear light height is roughly **halfway up the car**, say ~800â€“900 mm from the ground.
- Car **length = 4480 mm**, so the **rear bumper to front bumper span** is known if needed.

---

## ğŸ§  **2. Extract Corresponding Image Points**
From the uploaded image:
- Identify and annotate pixel coordinates of:
  - Rear-left light center â†’ L
  - Rear-right light center â†’ R
  - Bottom of wheels (for ground contact)
  - Roofline midpoint (for vertical context)
- Optional: license plate corners or edges of the bumper for extra symmetry cues.

---

## ğŸ” **3. Back-Project to 3D**
Assuming you have the **camera calibration matrix K**, for each image point \( p \) (in homogeneous coordinates):
```math
\text{Ray direction} = K^{-1} p
```
You get the **3D ray** from the camera through the light points. You need at least **one depth estimate** to localize them in 3D â€” here's where your prior model dimensions help.

---

## ğŸ“ **4. Estimate the Ground Plane (Ï€)**
Using:
- The 3D directions of the rear lights (from rays)
- Their known real-world separation
- The assumption that the rear lights lie on a vertical plane
You can:
- Estimate Ï€ using triangulation (for car translating case), or
- Use the **vanishing points** method from the slides (Space Occupancy PDF)

---

## ğŸ” **5. Compute Space Occupied**
Once Ï€ and the 3D rear points are known:
- Reconstruct a rectangle or quadrilateral approximating the vehicle's footprint.
- This is your **occupied space on the road** in each frame.

---

## âš™ï¸ Optional Enhancements
- Use **Symmetric Feature Localization** from the slides for better pose estimation under low perspective.
- Use multiple frames to track light motion and deduce forward/curved motion (as outlined in the â€œCar translatingâ€ vs â€œCar steeringâ€ cases).

---